{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNLTGEX59ukG8Qf5hA8TwVZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hauduong05/NLP_basics/blob/main/machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxNhpcvqXwS8",
        "outputId": "b3ed38ec-35f3-46c5-e511-ac8912ce0daa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5MLgVoeX8z-",
        "outputId": "309572a9-bf0b-49db-b140-3640d917d9e0"
      },
      "source": [
        "cd /content/drive/MyDrive/NMT_data/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NMT_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ3QiygNBkjE"
      },
      "source": [
        "import unicodedata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SpRGbf6dtol"
      },
      "source": [
        "pad_token = 0\n",
        "sos_token = 1\n",
        "eos_token = 2\n",
        "\n",
        "class Vocab():\n",
        "  def __init__(self, name):\n",
        "    super(Vocab, self).__init__()\n",
        "    self.name = name\n",
        "    self.word2idx = {}\n",
        "    self.word2count = {}\n",
        "    self.idx2word = {0:'pad', 1:'sos', 2:'eos'}\n",
        "    self.n_words = 3\n",
        "    self.max_length = 0\n",
        "  \n",
        "  def addSentence(self, sentence):\n",
        "    words = sentence.split(\" \")\n",
        "    self.max_length = max(self.max_length, len(words))\n",
        "    for word in words:\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2idx:\n",
        "      self.word2idx[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.idx2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXTd09uRYBDU"
      },
      "source": [
        "def no_space(char, prev_char):\n",
        "  return char in ',.;?!' and prev_char != ' '\n",
        "\n",
        "def read_data(file):\n",
        "  with open(file, encoding='utf-8') as f:\n",
        "    return f.read()\n",
        "\n",
        "def preprocessing(data, pad_token):\n",
        "  inp, tar = [], []\n",
        "  data = unicodedata.normalize('NFKD', data)\n",
        "  data = ''.join([' ' + char if i > 0 and no_space(char, data[i - 1]) else char for i, char in enumerate(data)])\n",
        "  lines = data.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "  for pair in pairs:\n",
        "    Eng.addSentence(pair[0])\n",
        "    Fra.addSentence(pair[1])\n",
        "  return pairs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCt6-NaLddHW"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.Embedding = nn.Embedding(input_size, self.hidden_size)\n",
        "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    output = self.Embedding(input).view(1, 1, -1)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gduj7r6Ofxir"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
        "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "    self.fc = nn.Linear(self.hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    output = self.embedding(input).view(1, 1, -1)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.softmax(self.fc(output[0]))\n",
        "    return output, hidden\n",
        "  \n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pGZHmilSRFa"
      },
      "source": [
        "def indexfromsentences(vocab, sentence):\n",
        "  return [vocab.word2idx[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorfromsentence(vocab, sentence):\n",
        "  idx = indexfromsentences(vocab, sentence)\n",
        "  idx.append(eos_token)\n",
        "  return torch.tensor(idx, dtype=torch.long).view(-1, 1)\n",
        "\n",
        "def tensorfrompair(pair):\n",
        "  input_tensor = tensorfromsentence(Eng, pair[0])\n",
        "  output_tensor = tensorfromsentence(Fra, pair[1])\n",
        "  return (input_tensor, output_tensor)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZTjk622D7YP"
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "  input_length = input_tensor.size(0)\n",
        "  target_length = target_tensor.size(0)\n",
        "  loss = 0\n",
        "\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  \n",
        "  encoder_hidden = encoder.init_hidden()\n",
        "  for i in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "\n",
        "  decoder_input = torch.tensor([[sos_token]])\n",
        "  decoder_hidden = encoder_hidden\n",
        "  for i in range(target_length):\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "    loss += criterion(decoder_output, target_tensor[i])\n",
        "    decoder_input = target_tensor[i]\n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss.item() / target_length"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKu0lkpGJgyk"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every = 100, plot_every = 100, learning_rate = 0.01):\n",
        "  print_loss_total = 0\n",
        "  plot_loss_total = 0\n",
        "  plot_losses = []\n",
        "\n",
        "  criterion = nn.NLLLoss()\n",
        "  encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "  decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "  training_pairs = [tensorfrompair(random.choice(pairs)) for i in range(n_iters)]\n",
        "  for iter in range(n_iters):\n",
        "    training_pair = training_pairs[iter]\n",
        "    input_tensor = training_pair[0]\n",
        "    target_tensor = training_pair[1]\n",
        "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    print_loss_total += loss\n",
        "    plot_loss_total += loss\n",
        "\n",
        "    if (iter + 1) % print_every == 0:\n",
        "      print_loss_avg = print_loss_total / print_every\n",
        "      print_loss_total = 0\n",
        "      print(f'iter : {iter+1} loss = {print_loss_avg:.3f}')\n",
        "\n",
        "    if (iter + 1) % plot_every:\n",
        "      plot_loss_avg = plot_loss_total / plot_every\n",
        "      plot_losses.append(plot_loss_avg)\n",
        "      plot_loss_total = 0\n",
        "\n",
        "  showPlot(plot_losses)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faXShYejTnEe"
      },
      "source": [
        "def showPlot(plot_losses):\n",
        "  plt.figure()\n",
        "  plt.plot(plot_losses)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt8bj9mmFlUS"
      },
      "source": [
        "data = read_data('eng-fra.txt')\n",
        "Eng = Vocab('eng')\n",
        "Fra = Vocab('fra')\n",
        "pairs = preprocessing(data, pad_token)\n",
        "hidden_size = 256"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z62lP7o9VmuT"
      },
      "source": [
        "encoder = Encoder(Eng.n_words, hidden_size)\n",
        "decoder = Decoder(hidden_size, Fra.n_words)\n",
        "trainIters(encoder, decoder, 50000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoXj_Uvn1DJa"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = tensorfromsentence(Eng, sentence)\n",
        "    input_length = input_tensor.size()[0]\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    decoded_word = []\n",
        "\n",
        "    for i in range(input_length):\n",
        "      encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "    \n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = torch.tensor([[sos_token]])\n",
        "    \n",
        "    for i in range(max_length):\n",
        "      decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "      topv, topi = decoder_output.data.topk(1)\n",
        "      if topi.item() == eos_token:\n",
        "        decoded_word.append('<EOS>')\n",
        "      else:\n",
        "        decoded_word.append(Fra.idx2word[topi.item()])\n",
        "      decoder_input = topi.squeeze().detach()\n",
        "  \n",
        "  return decoded_words"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}